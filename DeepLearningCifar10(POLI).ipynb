{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YgorAlcantara/DeepLearningCifar10/blob/main/DeepLearningCifar10(POLI).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u_w4ruhJcTNZ"
      },
      "outputs": [],
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 120\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "sAzBc2QDcVgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09d3f938-d133-45bc-cbe2-768f3fb56269"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 14s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y39nL1HdccTp",
        "outputId": "5c5bccef-7514-4cc8-99e0-8658d27bc38c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert class vectors to binary class matrices\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "tz9ZfpHxcfBI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "# prepare iterator\n",
        "it_train = datagen.flow(x_train, y_train, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "36-SwT6GL-Cw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# compile model\n",
        "opt = Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "callbacks = [\n",
        "    #EarlyStopping(monitor='loss', patience=3),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
        "]\n",
        "\n",
        "steps = int(x_train.shape[0] / batch_size)\n",
        "model.fit(it_train, steps_per_epoch=steps, epochs=epochs, validation_data=(x_test, y_test), verbose=1, callbacks=callbacks)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "l919knBmcjHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "363f2731-2a17-4b12-db8b-9dbfef329df0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "781/781 [==============================] - 45s 45ms/step - loss: 1.7538 - accuracy: 0.4154 - val_loss: 1.7240 - val_accuracy: 0.4248 - lr: 0.0100\n",
            "Epoch 2/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.0466 - accuracy: 0.6291 - val_loss: 0.8496 - val_accuracy: 0.7103 - lr: 0.0100\n",
            "Epoch 3/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.8521 - accuracy: 0.7054 - val_loss: 0.7538 - val_accuracy: 0.7495 - lr: 0.0100\n",
            "Epoch 4/120\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.7443 - accuracy: 0.7454 - val_loss: 0.8521 - val_accuracy: 0.7339 - lr: 0.0100\n",
            "Epoch 5/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.6803 - accuracy: 0.7675 - val_loss: 0.7161 - val_accuracy: 0.7655 - lr: 0.0100\n",
            "Epoch 6/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.6226 - accuracy: 0.7882 - val_loss: 0.8337 - val_accuracy: 0.7392 - lr: 0.0100\n",
            "Epoch 7/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.5942 - accuracy: 0.7995 - val_loss: 1.9514 - val_accuracy: 0.5524 - lr: 0.0100\n",
            "Epoch 8/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.5494 - accuracy: 0.8152 - val_loss: 0.6392 - val_accuracy: 0.8015 - lr: 0.0100\n",
            "Epoch 9/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5205 - accuracy: 0.8247 - val_loss: 0.5600 - val_accuracy: 0.8137 - lr: 0.0100\n",
            "Epoch 10/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.5135 - accuracy: 0.8278 - val_loss: 0.5289 - val_accuracy: 0.8342 - lr: 0.0100\n",
            "Epoch 11/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.4912 - accuracy: 0.8349 - val_loss: 0.7959 - val_accuracy: 0.7537 - lr: 0.0100\n",
            "Epoch 12/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.4710 - accuracy: 0.8419 - val_loss: 0.5008 - val_accuracy: 0.8425 - lr: 0.0100\n",
            "Epoch 13/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.4328 - accuracy: 0.8547 - val_loss: 2.2687 - val_accuracy: 0.4620 - lr: 0.0100\n",
            "Epoch 14/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.4382 - accuracy: 0.8524 - val_loss: 0.4798 - val_accuracy: 0.8470 - lr: 0.0100\n",
            "Epoch 15/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.4042 - accuracy: 0.8642 - val_loss: 0.4754 - val_accuracy: 0.8480 - lr: 0.0100\n",
            "Epoch 16/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3911 - accuracy: 0.8694 - val_loss: 0.4287 - val_accuracy: 0.8642 - lr: 0.0100\n",
            "Epoch 17/120\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.3729 - accuracy: 0.8747 - val_loss: 0.4436 - val_accuracy: 0.8650 - lr: 0.0100\n",
            "Epoch 18/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3605 - accuracy: 0.8789 - val_loss: 0.4206 - val_accuracy: 0.8713 - lr: 0.0100\n",
            "Epoch 19/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3515 - accuracy: 0.8822 - val_loss: 0.3925 - val_accuracy: 0.8782 - lr: 0.0100\n",
            "Epoch 20/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3356 - accuracy: 0.8885 - val_loss: 0.4145 - val_accuracy: 0.8728 - lr: 0.0100\n",
            "Epoch 21/120\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.3614 - accuracy: 0.8797 - val_loss: 0.5850 - val_accuracy: 0.8420 - lr: 0.0100\n",
            "Epoch 22/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.3192 - accuracy: 0.8932 - val_loss: 0.5158 - val_accuracy: 0.8518 - lr: 0.0100\n",
            "Epoch 23/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.3027 - accuracy: 0.8988 - val_loss: 0.5513 - val_accuracy: 0.8443 - lr: 0.0100\n",
            "Epoch 24/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2950 - accuracy: 0.9014 - val_loss: 0.4319 - val_accuracy: 0.8760 - lr: 0.0100\n",
            "Epoch 25/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.2161 - accuracy: 0.9260 - val_loss: 0.3098 - val_accuracy: 0.9063 - lr: 0.0020\n",
            "Epoch 26/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.1898 - accuracy: 0.9351 - val_loss: 0.3058 - val_accuracy: 0.9081 - lr: 0.0020\n",
            "Epoch 27/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.1757 - accuracy: 0.9402 - val_loss: 0.3104 - val_accuracy: 0.9079 - lr: 0.0020\n",
            "Epoch 28/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.1711 - accuracy: 0.9422 - val_loss: 0.3013 - val_accuracy: 0.9117 - lr: 0.0020\n",
            "Epoch 29/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.1601 - accuracy: 0.9451 - val_loss: 0.3093 - val_accuracy: 0.9092 - lr: 0.0020\n",
            "Epoch 30/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.1540 - accuracy: 0.9479 - val_loss: 0.2938 - val_accuracy: 0.9143 - lr: 0.0020\n",
            "Epoch 31/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.1532 - accuracy: 0.9468 - val_loss: 0.3158 - val_accuracy: 0.9111 - lr: 0.0020\n",
            "Epoch 32/120\n",
            "781/781 [==============================] - 32s 42ms/step - loss: 0.1435 - accuracy: 0.9509 - val_loss: 0.2930 - val_accuracy: 0.9162 - lr: 0.0020\n",
            "Epoch 33/120\n",
            "781/781 [==============================] - 32s 40ms/step - loss: 0.1389 - accuracy: 0.9520 - val_loss: 0.3090 - val_accuracy: 0.9148 - lr: 0.0020\n",
            "Epoch 34/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.1317 - accuracy: 0.9540 - val_loss: 0.3018 - val_accuracy: 0.9165 - lr: 0.0020\n",
            "Epoch 35/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.1302 - accuracy: 0.9547 - val_loss: 0.2955 - val_accuracy: 0.9173 - lr: 0.0020\n",
            "Epoch 36/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.1277 - accuracy: 0.9560 - val_loss: 0.3017 - val_accuracy: 0.9161 - lr: 0.0020\n",
            "Epoch 37/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.1243 - accuracy: 0.9575 - val_loss: 0.3009 - val_accuracy: 0.9176 - lr: 0.0020\n",
            "Epoch 38/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.1145 - accuracy: 0.9605 - val_loss: 0.2997 - val_accuracy: 0.9188 - lr: 0.0010\n",
            "Epoch 39/120\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.1122 - accuracy: 0.9609 - val_loss: 0.3011 - val_accuracy: 0.9158 - lr: 0.0010\n",
            "Epoch 40/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.1076 - accuracy: 0.9630 - val_loss: 0.2900 - val_accuracy: 0.9177 - lr: 0.0010\n",
            "Epoch 41/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.1028 - accuracy: 0.9636 - val_loss: 0.2906 - val_accuracy: 0.9203 - lr: 0.0010\n",
            "Epoch 42/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.1000 - accuracy: 0.9649 - val_loss: 0.2947 - val_accuracy: 0.9192 - lr: 0.0010\n",
            "Epoch 43/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.0996 - accuracy: 0.9651 - val_loss: 0.3043 - val_accuracy: 0.9201 - lr: 0.0010\n",
            "Epoch 44/120\n",
            "781/781 [==============================] - 32s 42ms/step - loss: 0.1007 - accuracy: 0.9649 - val_loss: 0.2897 - val_accuracy: 0.9215 - lr: 0.0010\n",
            "Epoch 45/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0974 - accuracy: 0.9669 - val_loss: 0.2959 - val_accuracy: 0.9206 - lr: 0.0010\n",
            "Epoch 46/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0971 - accuracy: 0.9663 - val_loss: 0.2953 - val_accuracy: 0.9212 - lr: 0.0010\n",
            "Epoch 47/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.0928 - accuracy: 0.9667 - val_loss: 0.2902 - val_accuracy: 0.9218 - lr: 0.0010\n",
            "Epoch 48/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0892 - accuracy: 0.9687 - val_loss: 0.3023 - val_accuracy: 0.9195 - lr: 0.0010\n",
            "Epoch 49/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.0870 - accuracy: 0.9694 - val_loss: 0.2910 - val_accuracy: 0.9212 - lr: 0.0010\n",
            "Epoch 50/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0924 - accuracy: 0.9680 - val_loss: 0.3028 - val_accuracy: 0.9197 - lr: 0.0010\n",
            "Epoch 51/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0854 - accuracy: 0.9700 - val_loss: 0.3000 - val_accuracy: 0.9214 - lr: 0.0010\n",
            "Epoch 52/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0874 - accuracy: 0.9694 - val_loss: 0.3138 - val_accuracy: 0.9194 - lr: 0.0010\n",
            "Epoch 53/120\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.0853 - accuracy: 0.9698 - val_loss: 0.3090 - val_accuracy: 0.9186 - lr: 0.0010\n",
            "Epoch 54/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.0833 - accuracy: 0.9708 - val_loss: 0.2989 - val_accuracy: 0.9215 - lr: 0.0010\n",
            "Epoch 55/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.0807 - accuracy: 0.9718 - val_loss: 0.3020 - val_accuracy: 0.9214 - lr: 0.0010\n",
            "Epoch 56/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0795 - accuracy: 0.9712 - val_loss: 0.3099 - val_accuracy: 0.9200 - lr: 0.0010\n",
            "Epoch 57/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0767 - accuracy: 0.9729 - val_loss: 0.3315 - val_accuracy: 0.9177 - lr: 0.0010\n",
            "Epoch 58/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0763 - accuracy: 0.9733 - val_loss: 0.3252 - val_accuracy: 0.9182 - lr: 0.0010\n",
            "Epoch 59/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0765 - accuracy: 0.9734 - val_loss: 0.3077 - val_accuracy: 0.9219 - lr: 0.0010\n",
            "Epoch 60/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0758 - accuracy: 0.9735 - val_loss: 0.3094 - val_accuracy: 0.9207 - lr: 0.0010\n",
            "Epoch 61/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.0777 - accuracy: 0.9727 - val_loss: 0.3095 - val_accuracy: 0.9195 - lr: 0.0010\n",
            "Epoch 62/120\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.0710 - accuracy: 0.9751 - val_loss: 0.3181 - val_accuracy: 0.9195 - lr: 0.0010\n",
            "Epoch 63/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0736 - accuracy: 0.9744 - val_loss: 0.3134 - val_accuracy: 0.9217 - lr: 0.0010\n",
            "Epoch 64/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0734 - accuracy: 0.9743 - val_loss: 0.3180 - val_accuracy: 0.9213 - lr: 0.0010\n",
            "Epoch 65/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0706 - accuracy: 0.9747 - val_loss: 0.3147 - val_accuracy: 0.9204 - lr: 0.0010\n",
            "Epoch 66/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0741 - accuracy: 0.9738 - val_loss: 0.3182 - val_accuracy: 0.9205 - lr: 0.0010\n",
            "Epoch 67/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0652 - accuracy: 0.9766 - val_loss: 0.3287 - val_accuracy: 0.9186 - lr: 0.0010\n",
            "Epoch 68/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0693 - accuracy: 0.9761 - val_loss: 0.3148 - val_accuracy: 0.9201 - lr: 0.0010\n",
            "Epoch 69/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0674 - accuracy: 0.9763 - val_loss: 0.3207 - val_accuracy: 0.9196 - lr: 0.0010\n",
            "Epoch 70/120\n",
            "781/781 [==============================] - 32s 42ms/step - loss: 0.0684 - accuracy: 0.9761 - val_loss: 0.3178 - val_accuracy: 0.9193 - lr: 0.0010\n",
            "Epoch 71/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0680 - accuracy: 0.9767 - val_loss: 0.3130 - val_accuracy: 0.9237 - lr: 0.0010\n",
            "Epoch 72/120\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.0636 - accuracy: 0.9777 - val_loss: 0.3197 - val_accuracy: 0.9213 - lr: 0.0010\n",
            "Epoch 73/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0652 - accuracy: 0.9774 - val_loss: 0.3225 - val_accuracy: 0.9225 - lr: 0.0010\n",
            "Epoch 74/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0619 - accuracy: 0.9781 - val_loss: 0.3223 - val_accuracy: 0.9246 - lr: 0.0010\n",
            "Epoch 75/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0608 - accuracy: 0.9783 - val_loss: 0.3283 - val_accuracy: 0.9230 - lr: 0.0010\n",
            "Epoch 76/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0643 - accuracy: 0.9780 - val_loss: 0.3232 - val_accuracy: 0.9231 - lr: 0.0010\n",
            "Epoch 77/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.0615 - accuracy: 0.9792 - val_loss: 0.3226 - val_accuracy: 0.9218 - lr: 0.0010\n",
            "Epoch 78/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.0597 - accuracy: 0.9787 - val_loss: 0.3342 - val_accuracy: 0.9216 - lr: 0.0010\n",
            "Epoch 79/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0616 - accuracy: 0.9781 - val_loss: 0.3232 - val_accuracy: 0.9230 - lr: 0.0010\n",
            "Epoch 80/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0585 - accuracy: 0.9799 - val_loss: 0.3244 - val_accuracy: 0.9222 - lr: 0.0010\n",
            "Epoch 81/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0588 - accuracy: 0.9795 - val_loss: 0.3293 - val_accuracy: 0.9221 - lr: 0.0010\n",
            "Epoch 82/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0593 - accuracy: 0.9794 - val_loss: 0.3248 - val_accuracy: 0.9237 - lr: 0.0010\n",
            "Epoch 83/120\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.0580 - accuracy: 0.9796 - val_loss: 0.3383 - val_accuracy: 0.9211 - lr: 0.0010\n",
            "Epoch 84/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0566 - accuracy: 0.9798 - val_loss: 0.3328 - val_accuracy: 0.9213 - lr: 0.0010\n",
            "Epoch 85/120\n",
            "781/781 [==============================] - 32s 42ms/step - loss: 0.0538 - accuracy: 0.9815 - val_loss: 0.3198 - val_accuracy: 0.9250 - lr: 0.0010\n",
            "Epoch 86/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0555 - accuracy: 0.9800 - val_loss: 0.3429 - val_accuracy: 0.9212 - lr: 0.0010\n",
            "Epoch 87/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0539 - accuracy: 0.9809 - val_loss: 0.3353 - val_accuracy: 0.9239 - lr: 0.0010\n",
            "Epoch 88/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.0524 - accuracy: 0.9814 - val_loss: 0.3442 - val_accuracy: 0.9215 - lr: 0.0010\n",
            "Epoch 89/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0516 - accuracy: 0.9818 - val_loss: 0.3424 - val_accuracy: 0.9222 - lr: 0.0010\n",
            "Epoch 90/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.0541 - accuracy: 0.9807 - val_loss: 0.3428 - val_accuracy: 0.9199 - lr: 0.0010\n",
            "Epoch 91/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0524 - accuracy: 0.9814 - val_loss: 0.3335 - val_accuracy: 0.9224 - lr: 0.0010\n",
            "Epoch 92/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0543 - accuracy: 0.9812 - val_loss: 0.3464 - val_accuracy: 0.9198 - lr: 0.0010\n",
            "Epoch 93/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0534 - accuracy: 0.9818 - val_loss: 0.3264 - val_accuracy: 0.9246 - lr: 0.0010\n",
            "Epoch 94/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.0527 - accuracy: 0.9816 - val_loss: 0.3352 - val_accuracy: 0.9221 - lr: 0.0010\n",
            "Epoch 95/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.0505 - accuracy: 0.9825 - val_loss: 0.3435 - val_accuracy: 0.9240 - lr: 0.0010\n",
            "Epoch 96/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0475 - accuracy: 0.9836 - val_loss: 0.3496 - val_accuracy: 0.9217 - lr: 0.0010\n",
            "Epoch 97/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0515 - accuracy: 0.9820 - val_loss: 0.3558 - val_accuracy: 0.9200 - lr: 0.0010\n",
            "Epoch 98/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0484 - accuracy: 0.9834 - val_loss: 0.3458 - val_accuracy: 0.9237 - lr: 0.0010\n",
            "Epoch 99/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.0506 - accuracy: 0.9825 - val_loss: 0.3672 - val_accuracy: 0.9188 - lr: 0.0010\n",
            "Epoch 100/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0466 - accuracy: 0.9842 - val_loss: 0.3579 - val_accuracy: 0.9231 - lr: 0.0010\n",
            "Epoch 101/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.0495 - accuracy: 0.9824 - val_loss: 0.3467 - val_accuracy: 0.9240 - lr: 0.0010\n",
            "Epoch 102/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0490 - accuracy: 0.9829 - val_loss: 0.3479 - val_accuracy: 0.9209 - lr: 0.0010\n",
            "Epoch 103/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.0494 - accuracy: 0.9820 - val_loss: 0.3722 - val_accuracy: 0.9169 - lr: 0.0010\n",
            "Epoch 104/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.0483 - accuracy: 0.9832 - val_loss: 0.3548 - val_accuracy: 0.9209 - lr: 0.0010\n",
            "Epoch 105/120\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.0479 - accuracy: 0.9835 - val_loss: 0.3564 - val_accuracy: 0.9198 - lr: 0.0010\n",
            "Epoch 106/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.0461 - accuracy: 0.9841 - val_loss: 0.3534 - val_accuracy: 0.9238 - lr: 0.0010\n",
            "Epoch 107/120\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.0472 - accuracy: 0.9836 - val_loss: 0.3611 - val_accuracy: 0.9190 - lr: 0.0010\n",
            "Epoch 108/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0452 - accuracy: 0.9840 - val_loss: 0.3572 - val_accuracy: 0.9212 - lr: 0.0010\n",
            "Epoch 109/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0457 - accuracy: 0.9843 - val_loss: 0.3652 - val_accuracy: 0.9194 - lr: 0.0010\n",
            "Epoch 110/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0459 - accuracy: 0.9842 - val_loss: 0.3577 - val_accuracy: 0.9204 - lr: 0.0010\n",
            "Epoch 111/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.0475 - accuracy: 0.9834 - val_loss: 0.3599 - val_accuracy: 0.9201 - lr: 0.0010\n",
            "Epoch 112/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0459 - accuracy: 0.9836 - val_loss: 0.3550 - val_accuracy: 0.9223 - lr: 0.0010\n",
            "Epoch 113/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.0427 - accuracy: 0.9851 - val_loss: 0.3635 - val_accuracy: 0.9210 - lr: 0.0010\n",
            "Epoch 114/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0449 - accuracy: 0.9849 - val_loss: 0.3676 - val_accuracy: 0.9209 - lr: 0.0010\n",
            "Epoch 115/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.0446 - accuracy: 0.9853 - val_loss: 0.3504 - val_accuracy: 0.9232 - lr: 0.0010\n",
            "Epoch 116/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0444 - accuracy: 0.9849 - val_loss: 0.3611 - val_accuracy: 0.9197 - lr: 0.0010\n",
            "Epoch 117/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.0418 - accuracy: 0.9853 - val_loss: 0.3567 - val_accuracy: 0.9218 - lr: 0.0010\n",
            "Epoch 118/120\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.0407 - accuracy: 0.9859 - val_loss: 0.3574 - val_accuracy: 0.9226 - lr: 0.0010\n",
            "Epoch 119/120\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.0413 - accuracy: 0.9856 - val_loss: 0.3584 - val_accuracy: 0.9246 - lr: 0.0010\n",
            "Epoch 120/120\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.0414 - accuracy: 0.9850 - val_loss: 0.3736 - val_accuracy: 0.9195 - lr: 0.0010\n",
            "Test loss: 0.37362298369407654\n",
            "Test accuracy: 0.9194999933242798\n"
          ]
        }
      ]
    }
  ]
}